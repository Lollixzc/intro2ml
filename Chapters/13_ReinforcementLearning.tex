\chapter{Reinforcement learning}

\section{Introduzione}
L'idea del Reinforcement learning \`e che si ha un agente e un ambiente: l'agente svolge azioni che cambiano l'ambiente e conseguentemente l'ambiente ritorna all'agente delle rewards.
Questa idea viene presa dalle strategie di animali:
\begin{itemize}
	\item L'ambiente si trova in uno stato $s$.
	\item L'agente svolge un'azione.
	\item L'azione modifica l'ambiente.
	\item L'ambiente ritorna una reward all'agente e il nuovo stato $s'$.
\end{itemize}

	\subsection{Policy}
	L'agente tenta di imparare una policy o un mapping da stati a azioni in modo da massimizzare la reward data dall'ambiente.
	
	\section{Markov decision process (MDP)}
	MDP utilizza l'assuzione di Markov in cui uno stato al tempo $t$ dipende solo dallo stato precedente $s(t-1)$ e dall'azione precedente $a(t-1)$ per trovare una policy.
	Questo utilizza l'equazione di Bellman e la programmazione dinamica.
	L'obiettivo \`e trovare una policy, una mappa che ritorna tutte le azioni ottimali di ogni stato sul nostro ambiente.
	
	\subsection{Componenti}
	\begin{itemize}
		\item Stati $s_i$ che iniziano con uno stato $s_0$.
		\item Azioni $a$.
		\item Modello di transizione $P(s'|s,a)$, secondo l'assunzione di Markov la probabilit\`a di arrivare a $s'$ da $s$ dipende solo da $s$ e non da nessuna delle azioni passate o stati passati.
		\item Reward function $r(s)$.
		\item Policy $\pi(s)$, l'azione che un agente svolge in ogni stato dato.
	\end{itemize}
	Si nota come una MDP \`e definita dalla tupla $(S, A, R, P, y)$, dove $S$ \`e l'insieme degli stati possibili, $A$ l'insieme delle azioni possibili, $R$ la distribuzione delle reward dato uno stato, $P$ la probabilit\`a di transizione o la distribuzione rispetto al prossimo stato dato $(stato, azione)$ e $y$ il discount factor.
	La reward viene utilizzata per ottenere la policy migliore.
	L'obiettivo \`e pertanto di trovare la policy ottimale.
	
	\subsection{Ambienti stocastici}
	MDP viene utilizzata tipicamente negli ambienti stocastici, pertanto lo stato a tempo $t+1$ non \`e deterministico quando si conosce lo stato $s_t$ e l'azione $a_t:s_{t+1}$ viene estratta dalla probabilit\`a di transizione e deriva da un processo stocastico.
	
	\begin{figure}
		\centering
		\begin{minipage}{.5\textwidth}
			\centering
			\includegraphics[width=0.7\linewidth]{imgs/chapter13/img0}
			\caption{Reinforcement learning}
			\label{fig:chapter13-00}
		\end{minipage}%
		\begin{minipage}{.5\textwidth}
			\centering
			\includegraphics[width=1\linewidth]{imgs/chapter13/img1}
			\caption{Esempi di ambienti}
			\label{fig:chapter13-01}
		\end{minipage}
	\end{figure}
	
	\subsection{MDP loop}
	\begin{itemize}
		\item A tempo $t=0$ l'ambiente si campiona nello stato iniziale $s_0\sim p(s_0)$.
		\item Si ripete:
		\begin{itemize}
			\item L'agente seleziona l'azione $a_i$.
			\item L'ambiente campiona la reward $r_t\sim R(\cdot|s_t, a_t)$.
			\item L'ambiente campiona il prossimo stato $s_{t+1}\sim P(\cdot|s_t, a_t)$
			\item L'agente riceve la reward $r_t$ e lo stato successivo $s_{t+1}$.
		\end{itemize}
	\end{itemize}
	
	\subsection{Policy}
	La policy $\pi(s)$ \`e la funzione da $S$ ad $A$ che specifica quale azione prendere in ogni stato.
	Si deve trovare la policy $\pi^*$ che massimizza cumulative discounted rewards.
	La reward viene data dalla funzione
	$$R:(stato, azione)\rightarrow reward$$

		\subsubsection{Cumulative discounted reward}
		Si supponga di avere una policy $\pi$ in uno stato di inizio $s_0$ che porta a una sequenza $s_0,\dots, s_n$.
		La cumulative reward della sequenza \`e:
		$$\sum\limits_{t\ge 0} r(s_t)$$
		\`E pertanto la somma delle reward di una serie di stati.
		Quella discounted \`e una cumulative reward che d\`a un peso a diverse reward in base al tempo dello step:
		$$\sum\limits_{t\ge 0} y^tr(s_t)\qquad\ where\ 0 < y\le 1$$
		Il discount factor \`e un modo di pesare l'importanza dei primi passi o dei passi futuri in accordo con il valore di $y^t$.
		Minore \`e il discount factor \`e minore l'importanza di reward future e l'agente tende a concentrarsi su azioni che portano a reward immediate.
		Questa strategia aiuta gli algoritmi a convergere.

\section{Confronto tra reinforcement learning e supervised learning}

	\subsection{Supervised learning loop}
	\begin{itemize}
		\item Prendi input $x_i$ campionato dalla distribuzione dei dati.
		\item Utilizza un modello con parametri $w$ per predirre l'output $y$.
		\item Osserva l'output obiettivo $y_i$ e loss $l(w, x_i, y_i)$.
		\item Aggiorna $w$ per ridurre la loss con SGD: $w = w -\eta\nabla l(w, x_i, y_i)$.
	\end{itemize}
	
	\subsection{Reinforcement learning loop}
	\begin{itemize}
		\item Dallo stato $s_i$ svolgi un azione $a$ determinata dalla policy $\pi(s)$.
		\item L'ambiente seleziona lo stato successivo $s'$ in base al modello di transizione $P(s'|s,a)$.
		\item Osserva $s'$ e la reward $r(s)$, aggiornando la policy.
	\end{itemize}
	
	\subsection{Differenze fondamentali}
	\begin{itemize}
		\item Nel SL i dati non dipendono dall'input precedente, mentre in RL le azioni dell'agente influiscono il prossimo input.
		\item In SL si ha una loss function che guida il modello ai parametri migliori e si pu\`o differenziare con rispetto ai pesi, mentre in RL la reward guida lo sviluppo del modello, ma non \`e differenziabile rispetto ai paramatri.
		\item In SL si trova supervisione rispetto ad ogni passo, mentre in RL le rewards potrebbero essere sparse.
	\end{itemize}

\section{Metodi di reinforcement learning}
Per RL si trovano due approcci principali:
\begin{itemize}
	\item Metodi basati sul valore: una value function $V(s)$ viene introdotta e l'agente vuole masssimizzarla.
	La value di ogni stato \`e la quanit\`a totale di reward un agente pu\`o aspettarsi di collezionare rispetto al futuro cominciando da uno stato dato.
	\item Metodo basato sulla policy: si definisce una policy da ottimizzare direttamente, questa definisce come l'agente si comporta nella forma di distribuzione di probabilit\`a tra le azioni da svolgere in uno stato dato.
	Una policy stocastica d\`a la distribuzione di probabilit\`a su azioni diverse: $\pi_\theta(s,a)\approx P(a|s)$.
	
\end{itemize}

	\subsection{Value based methods}
	La funzione di value \`e una funzione dello stato con rispetto a una certa policy $\pi$.
	Ritorna la quantit\`a totale di reward che l'agente pu\`o aspettarsi da uno stato particolare a tutti gli stati possibili a partire da quello.
	Attraverso la value si pu\`o trovare una policy.
	La value function $V$ di uno stato con rispetto a una policy $\pi$ \`e l'aspettata cumulative reward di seguire tale policy cominciando in $s$:
	$$V^\pi(s) = \mathbb{E}\biggl[\sum\limits_{t\ge 0} y^tr(s_t)|s_0 = s,\pi\biggr]$$
	Con $a_t = \pi(s_t), s_{t+1}\sim P(\cdot|s_t, a_t)$.
	Il valore ottimale di uno stato \`e il valore raggiungibile seguendo la migliore policy:
	$$V^*(s) = \max_\pi\mathbb{E}\biggl[\sum\limits_{t\ge 0} y^tr(s_t)|s_0 = s,\pi\biggr]$$
	Dice pertanto quanto \`e buono uno stato.
	Si trova il valore atteso in quanto la sequenza di stati non \`e deterministica e non si ha accesso alla probabilit\`a di transizione.
	Rappresenta il valore che l'agente pu\`o aspettarsi da tutti gli stati possibili cominciando dallo stato iniziale $s$.

		\subsubsection{Q value function}
		Invece di gestire con la value function si utilizza la $Q$-value function che lavora con uno stato $s$, una azione $a$ e una policy $\pi$.
		Viene definita come:
		$$Q^\pi(s,a) = \mathbb{E}\biggl[\sum\limits_{t\ge 0} y^tr(s_t)|s_0 = s,a_0 = a\pi\biggr]$$
		Il valore ottimale di $Q$ dice quando \`e buona una coppia stato azione:
		$$Q^*(s,a) =\max_\pi\mathbb{E}\biggl[\sum\limits_{t\ge 0} y^tr(s_t)|s_0 = s,a_0 = a\pi\biggr]$$
		La Q value ottimale viene usata per computare la policy ottimale:
		$$\pi^*(s) = arg\max_aQ^*(s,a)$$
		La formula risultate della $Q$ value function \`e nella formula di un'equazione di Bellman:
		\begin{align*}
			Q^*(s,a) &= r(s) + y\sum\limits_{s'}P(s'|s,a)\max_{a'}Q^*(s',a')\\
			&=\mathbb{E}_{s'\sim P(\cdot|s,a)}[r(s)+y\max_{a'}Q^*(s',a')|s,a]
		\end{align*}
		Se il valore ottimale della coppia stato azione per il prossimo passo $Q^*(s',a')$ sono conosciuti allora la strategia ottima \`e svolgere l'azione che massimizza il valore atteso.
		Per calcolare il valore $Q$ dello stato corrente si deve calcolare il valore $Q$ degli stati vicini e cos\`i via con ricorsione.
		
		\subsubsection{Algoritmo Q-learning}
		Lo scopo di questo algoritmo \`e che la matrice di value $R$ \`e conosciuta unicamente all'ambiente e l'agente deve impararla con l'esperienza.
		L'agente possiede una matrice $Q$ che codifica stato, azione e rewards, ma \`e inizializzata a $0$ e diventa $R$ attraverso l'esperienza.
		La policy si ottiene attraverso questa matrice.
		\begin{enumerate}
			\item Inizializza la matrice $Q$ con zero.
			\item Seleziona uno stato iniziale casuale.
			\item Per ogni episodio (insieme di azioni che parte dallo stato iniziale e arriva allo stato finale).
			\item Mentre lo stato non \`e lo stato obiettivo.
			\item Seleziona una possibile azione casuale per lo stato corrente.
			\item Utilizzando l'azione considera arrivare allo stato prossimo.
			\item Ottieni il valore di $Q$ massimo per il prossimo stato.
			\item $Q^*(s,a) = R(s,a)+y\max\limits_a[Q^*(s',a')]$.
		\end{enumerate}
		Per trovare la policy ottimale:
		\begin{enumerate}
			\item Si pone lo stato corrente a quello iniziale.
			\item Dallo stato corrente si trova l'azione con il maggiore $Q$ value.
			\item Si pone lo stato corrente al prossimo.
			\item Si ripetono i passi $2$ e $3$ fino a che si raggiunge lo stato obiettivo.
		\end{enumerate}

		\subsubsection{Deep Q learning}
		L'equazione di Bellman pu\`o pertanto essere usate per imparare tabelle $Q$ per ritornare la policy ottimale.
		Nel mondo reale per\`o gli stati e le rewards possono essere troppo grandi per essere calcolate completamente.
		Per questa ragione viene introdotto un algoritmo di deep $Q$ learning che sfrutta l'idea di approssimare $Q$ utilizzando una funzione parametrica trainata da una NN.
		Si pu\`o approssimare $Q$ a
		$$Q^*(s,a) \approx Q_w(s,a)$$
		In quanto $Q_w$ \`e una funzione dei pesi $w$ si pu\`o trainare una NN per approssimare $Q_w$.
		Ad ogni interazione del training si aggiornano i parametri $w$ del modello per avvicinare $Q$ a $y$:
		$$y_i(s,a) = \mathbb{E}_{s'\sim P(\cdot|s,a)}[r(s)+y\max_{a'}Q_{w_{i-1}}(s',a')|s,a]$$
		La loss function
		$$L_i(w_i) = \mathbb{E}_{s,a\sim \rho}[(y_i(s,a) - Q_{w_i}(s,a))^2]$$
		Dove $\rho$ \`e la distribuzione di probabilit\`a sugli stati $s$ e le azioni $a$ che si riferiscono come distribuzione di comportamento.
		Si definisce a ogni iterazione $i$ e un target $y_i$ ottenuti attraverso la Bellman equation.
		Dati questi valori si pu\`o derivare $L_i$.
		Aggiornamento del gradiente:
		\begin{align*}
			\nabla_{w_i}L(w_i) &= \mathbb{E}_{s,a\sim\rho}[(y_i(s,a) - Q_{w_i}(s,a))\nabla_{w_i}Q_{w_i}(s,a)]=\\
			&= \mathbb{E}_{s,a\sim\rho,s'}[(r(s) + y\max_{a'}Q_{w_{i-1}}(s',a')-Q_{w_i}(s,a))\nabla_{w_i}Q_{w_i}(s,a)]
		\end{align*}
		Attraverso il training SGD si sostituisce il valore atteso campionando l'esperienza $(s,a,s')$ utilizzando la distribuzione di comportamento e il modello di transizione.
		
		\paragraph{Problematiche}
		Il training \`e prono all'instabilit\`a e a differenza del supervised learning i target si muovono.
		Esperienze successive sono correlate e dipendenti dalla policy che pu\`o cambiare rapidamente con piccoli cambi nei parametri, portando a un cambio drastico nella distribuzione dei dati.
		Per risolverli si pu\`o fare freezing sulla target Q network per aumentare la stabilit\`a della rete e utilizzare un experience replay: un buffer per salvare esperienze e campionare da quello in modo da seguire una strategia greedy che permette un trade-off tra exploitation ed esploration.

	\subsection{Policy gradient methods PGM}
	L'obiettivo dei PGM \`e di definire parametri che influenzano la policy $\pi$ e di impararli direttamente.
	Questa \`e la soluzione pi\`u semplice nel caso in cui si abbia uno spazio degli stati grande e continuo.
	L'idea \`e di utilizzare deep neural network per imparare la policy.
	Si deve pertanto imparare una funzione che ritorna la distribuzione di probabilit\`a di azioni rispetto allo stato corrente:
	$$\pi_\theta(s,a)\approx P(a|s)$$
	Si noti come questa ha un significato diverso rispetto ai value-based method: \`e una probabilit\`a di azioni rispetto agli stati, non una funzione da azioni a stati.
	In questo caso non si hanno le labels in modo da dare feedback alla NN attraverso backpropagation e viene usata la reward.
	Si devono pertanto trovare i migliori parametri $\theta$ della policy per massimizzare la reward aspettata attraverso gradient descent:
	\begin{align*}
		J(\theta) &= \mathbb{E}\biggl[\sum\limits_{t\ge 0}y^tr_t|\pi_\theta\biggr]\\
		&=\mathbb{E}_\tau[r(t)]
	\end{align*}
	Si vuole il valore atteso di ritorno di traiettorie: $\tau(s_0,a_0,r_0, \dots, s_n,a_n,r_n)$
	$$J(\theta) = \int_\tau r(\tau)p(\tau,\theta)d\tau$$
	Dove $p(\tau,\theta)$ \`e la probabilit\`a della traiettoria $\tau$ sotto la policy con i parametri $\theta$:
	$$p(\tau,\theta) = \prod\limits_{t\ge 0}\pi_\theta(s_t,a_t)P(s_{t+1}|s_t,a_t)$$
	Le traiettorie sono una semplificazione, un oggetto che rappresenta stato, azione e reward.
	In quanto l'integrale \`e di difficile risoluzione si differenzia utilizzando la log-trasformation:
	$$\nabla_\theta J(\theta) =\mathbb{E}_\tau[r(t)\nabla_\theta\log p(\tau,\theta)]$$
	Dove
	$$\log p(\tau,\theta) = \sum\limits_{t\ge 0} [\log \pi_\theta(s_t,a_t) + \log P(s_{t+1}|s_t,a_t)]$$
	E
	$$\nabla_\theta \log p(\tau,\theta) = \sum\limits_{t\ge 0}\nabla_\theta \log\pi_\theta(s_t,a_t)$$
	In questo modo si pu\`o computare il gradiente senza conoscere la probabilit\`a di transizione $p$.
	$$\nabla_\theta J(\theta) = \mathbb{E}_\tau\biggl[\biggl(\sum\limits_{t\ge 0}y^tr_t\biggr)\biggl(\sum\limits_{t\ge 0}\nabla_\theta\log\pi_\theta(s_t,a_t)\biggr)\biggr]$$
	Si pu\`o inoltre evitare di computare tutte le traiettorie possibili e usare unicametne un'approssimazione stocastica a $N$ traiettorie.
	$$\nabla_\theta J(\theta) \approx \dfrac{1}{N}\sum\limits_{i=1}^n\biggl(\sum\limits_{t = 0}^{T_i}y^tr_{i,t}\biggr)\biggl(\sum\limits_{t= 0}^{T_i}\nabla_\theta\log\pi_\theta(s_{i,t},a_{i,t})\biggr)$$

		\subsubsection{Reinforce}
		Reinforce \`e un algoritmo per deep RL che aggiorna i parametri:
		\begin{enumerate}
			\item Campiona $N$ traiettorie $\tau_i$ utilizzando la policy corrente $\pi_\theta$.
			\item Stima il gradiente di policy $\nabla_\theta J(\theta)$.
			\item Aggiorna i parametri attraverso gradient ascent $\theta = \theta + \eta\nabla_\theta J(\theta)$.
		\end{enumerate}
		
		\subsubsection{Single step reinforce}
		Single step reinforce \`e una variante del reinforce che stima il gradiente unicamente su uno step e non sull'intera traiettoria:
		\begin{enumerate}
			\item In uno stato $s$ campiona l'azione $a$ utilizzando la policy corrente $\pi_\theta$ e si osserva la reward $r$.
			\item Stima il gradiente $\nabla_\theta J(\theta)\sim r\nabla_\theta\log\pi_\theta(s,a)$
			\item Aggiorna i parametri attraverso gradient ascent $\theta = \theta + \eta\nabla_\theta J(\theta)$.
		\end{enumerate}
		Si nota come $r$ alto aumenta la probabilit\`a dell'azione vista, mentre se \`e basso si abbassa la probabilit\`a.
		Si aggiorna il parametri $\theta$ in accordo con il gradiente in quanto si vuole aumentare la reward o gradient ascent.
